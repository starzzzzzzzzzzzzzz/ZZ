# 系统架构概述

## 1. 系统定位
本项目是一个基于本地部署的智能知识库系统，允许用户创建和管理个性化的知识库，通过文档投喂的方式构建专业领域的知识体系，并支持与知识库进行智能对话交互。

## 2. 核心架构

### 2.1 导师（Mentor）层
- 基于本地部署的大语言模型（DeepSeek-R1-Distill-Qwen-32B-Q4_K_M）
- 通过LM Studio进行部署和管理
- 负责整体对话流程控制和知识整合
- 处理用户输入，协调知识检索和回答生成

### 2.2 专家（Expert）层
- 基于文档构建的知识库
- 使用ChromaDB进行向量存储和检索
- 支持多种格式文档的导入和处理
- 可持续扩展和更新知识内容
- 每个知识库代表一个特定领域的"专家"

### 2.3 特种专家（Special Expert）层
- 针对特定任务的专业模型
- 在MVP阶段暂不实现
- 未来可扩展用于处理特殊领域任务

## 3. 技术栈
- 后端框架：FastAPI
- 向量数据库：ChromaDB
- 模型服务：LM Studio
- 框架：LangChain
- 开发语言：Python 3.10

## 4. 数据流转
1. 用户提问
   - FastAPI接收用户请求
   
2. 导师处理
   - LangChain构建初始提示词模板
   - 引导模型进行问题分析和总结
   - 调用本地模型API
   - 生成检索摘要
   
3. 专家回答
   - LangChain处理层转换检索查询
   - ChromaDB执行向量检索
   - 按相关性返回检索结果
   
4. 导师补充
   - 整合检索结果
   - 生成最终答案
   - 返回处理后的回答

## 5. 核心特性
- 本地化部署，保证数据安全和响应速度
- "数据投喂"机制，支持知识库持续成长
- 文档和对话碎片分离存储
- 知识库的个性化构建
- 支持多种格式文档处理
- 语义化检索和智能问答

## 6. MVP阶段目标
- 实现基础的知识库管理功能
- 完成文档投喂和向量化存储
- 建立基本的问答流程
- 确保核心功能的稳定性和可用性 